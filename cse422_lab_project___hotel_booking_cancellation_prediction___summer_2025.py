# -*- coding: utf-8 -*-
"""CSE422 Lab Project | Hotel Booking Cancellation Prediction | Summer 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11SQcfG3xc8jbLNkGH7DFNBPPmi3GKrJe

# **1. Importing necessary libraries**
"""

# Import necessary libraries
print(f"Importing necessary libraries: Initiated")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from io import StringIO
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Supervised Learning Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

# Unsupervised Learning Model
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Metrics for evaluation
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    confusion_matrix,
    roc_curve,
    auc,
    classification_report,
)

print(f"Importing necessary libraries: Completed")

"""# **2. Dataset Description**

## 1.1 Dataset loading
"""

print("Dataset loading started.....")

def loadGoogleDriveDataset(fileID):
    url = f'https://drive.google.com/uc?export=download&id={fileID}'
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes
        csvData = StringIO(response.text)
        df = pd.read_csv(csvData)
        print("Dataset loaded successfully.")
        return df
    except requests.exceptions.RequestException as e:
        print(f"Error downloading the file: {e}")
        print("Please check if the Google Drive link is correct or not, and sharing is set to 'Anyone with the link'.")
        return None

googleDriveFileID = '1VQ3ww4sMHWxJvVtV2RA0Y025VPguSN9T'
df = loadGoogleDriveDataset(googleDriveFileID)

# check if data loading failed
if df is None:
    exit()

df.head(10)

df.shape

df.info()

"""## 1.2 Dataset description

1.2.0 Dataset description
"""

#FeatureCount
print(f"ðŸ”´ How many features? How many data points?")
print(f"The dataset has {df.shape[0]} data points (rows).")
print(f"The dataset has {df.shape[1]} features (cols).")
print(f"The dataset has {df.size} data-entries (rows x cols).")
print("")

#ClassificationORRegression
print(f"ðŸ”´ Classification or regression problem? ")
print(f"This is a binary-classification problem.")
print(f"Reason: The goal is to predict a binary outcome - whether a booking is canceled (1) or not (0).")
print()

# What kind of features are in your dataset?
print(f"ðŸ”´ What kind of features are in your dataset? ")
categoricalFeatures = df.select_dtypes(include=['object']).columns.tolist()
quantitativeFeatures = df.select_dtypes(include=np.number).columns.tolist()
print(f"Categorical features (inital): {categoricalFeatures}")
print(f"Quantitative features (initial): {quantitativeFeatures}")
print()

print(f"ðŸ”´ Do you need to encode the categorical variables?")
print("Categorical variables need to be encoded into a numerical format for machine learning algorithms to process them effectively.")
print()

"""1.2.1 Dataset visualization"""

print(f"Transposed stats for numerical features")
numericalFeatures = df.select_dtypes(include=np.number)
numericalFeatures.describe().T

print(f"Transposed stats for categorical features")
categoricalFeatures = df.select_dtypes(include=['object'])
categoricalFeatures.describe().T

numericalFeatures.hist(figsize=(12,12),bins=20)
plt.show()

"""1.2.2 Correlation Matrix"""

print("Generating correlation heatmap for all numerical features...")
plt.figure(figsize=(18, 12))

numericFeatures = df.select_dtypes(include=np.number)
corMatrix = numericFeatures.corr()
sns.heatmap(corMatrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 8})
plt.title('Correlation Matrix of Numerical Features', fontsize=16)
plt.show()

print("ðŸ”´ After observing the heatmap, 'is_canceled' shows notable correlations with 'lead_time', 'previous_cancellations', 'adr', and 'total_of_special_requests'.")

"""## 1.3 Imbalance Dataset Checking"""

#ImbalancedCheck
print("Checking for imbalanced dataset...")
print()
count = df['is_canceled'].value_counts()
percentage = df['is_canceled'].value_counts(normalize=True) * 100
classDistribution = pd.DataFrame({'count': count, 'percentage': percentage})
print(classDistribution)

# Represent using a bar chart
plt.figure(figsize=(6, 4))
sns.countplot(x='is_canceled', data=df, palette='viridis')
plt.title('Distribution of Booking Status')
plt.xlabel('Booking Status (0: Not Canceled, 1: Canceled)')
plt.ylabel('Number of Bookings')
plt.xticks([0, 1], ['Not Canceled', 'Canceled'])
plt.show()

print()
print("ðŸ”´ The bar chart shows an imbalance; there are more non-canceled bookings than canceled ones.")

"""# **3. Dataset Pre-processing**

## 3.1 Faults: Null / Missing values
"""

print("Handling missing values...")
print()

print(f"Dropping columns:")
# print(df.columns)
print(f"The company column has a very high percentage of missing values. It's better to drop it.")
df.drop('company', axis=1, inplace=True)
# Data Leakage
print("Dropping columns ('reservation_status', 'reservation_status_date') that would cause data leakage...")
df.drop(['reservation_status', 'reservation_status_date'], axis=1, inplace=True)
# Fault: Invalid Data
# Some bookings have 0 adults, 0 children, and 0 babies, which are invalid.
print("Removing invalid rows (0 total guests)...")
df.drop(df[(df['adults'] == 0) & (df['children'] == 0) & (df['babies'] == 0)].index, inplace=True)


print()
print("Filling Missing Values:")
print(f"For 'agent', NULL likely means no agent was involved. We can fill with 0 as a placeholder.")
df['agent'].fillna(0, inplace=True)
print(f"For 'country', the number of missing values is small. We'll fill with the mode (most frequent value).")
df['country'].fillna(df['country'].mode()[0], inplace=True)
print(f"For 'children', we'll fill the few missing values with the median (0).")
df['children'].fillna(df['children'].median(), inplace=True)

"""## 3.2 Feature Scaling"""

# Define features (X) and target (y)
x = df.drop('is_canceled', axis=1)
y = df['is_canceled']

# Identify final categorical and numerical features for the modeling pipeline
categoricalFeatures = x.select_dtypes(include=['object']).columns
numericalFeatures = x.select_dtypes(include=np.number).columns
print(f"Preprocessing complete. Features have been cleaned and selected for modeling.")

df.head(10)

"""# **4. Dataset Splitting**"""

# We will use an 80% train and 20% test split.
xTrain, xTest, yTrain, yTest = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Dataset split into training set ({len(xTrain)} rows) and testing set ({len(xTest)} rows) using stratification.")


# Pipeline for numerical features: Scale them using StandardScaler.
numericalTransformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Pipeline for categorical features: One-hot encode them.
categoricalTransformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', categories=[xTrain[col].unique() for col in categoricalFeatures]))
])

# Create a master preprocessor object using ColumnTransformer to apply different transformations to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numericalTransformer, numericalFeatures),
        ('cat', categoricalTransformer, categoricalFeatures)
    ],
    remainder='passthrough'
)

"""# **5. Model Training & Testing (Supervised)**"""

# Define the models as required by the instructions (Neural Network + at least 2 others)
# We will use 5 common and effective models for a thorough comparison.
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'),
    "Decision Tree": DecisionTreeClassifier(random_state=42, max_depth=10),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Neural Network (MLP)": MLPClassifier(max_iter=500, random_state=42, hidden_layer_sizes=(100, 50), activation='relu', solver='adam', early_stopping=True)
}

results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")

    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])

    # Train the model
    pipeline.fit(xTrain, yTrain)

    # Make predictions
    yPred = pipeline.predict(xTest)

    # For ROC curve, we need probability scores
    if hasattr(pipeline, "predict_proba"):
        yPredProb = pipeline.predict_proba(xTest)[:, 1]
    else: # For models like SVC without probability=True
        yPredProb = pipeline.decision_function(xTest)


    # Evaluate the model
    accuracy = accuracy_score(yTest, yPred)
    precision = precision_score(yTest, yPred)
    recall = recall_score(yTest, yPred)
    cm = confusion_matrix(yTest, yPred)
    fpr, tpr, _ = roc_curve(yTest, yPredProb)
    roc_auc = auc(fpr, tpr)

    # Store results, including the trained pipeline for later use
    results[name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'cm': cm,
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc,
        'pipeline': pipeline  # Store the entire trained pipeline
    }
    print(f"{name} training and evaluation complete.")

"""# **6. Model Comparison analysis**

## 6.1 Model Prediction Accuracy Comparison
"""

# Bar chart showcasing prediction accuracy
accuracies = {name: res['accuracy'] for name, res in results.items()}
plt.figure(figsize=(10, 6))
sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette='magma')
plt.title('Model Prediction Accuracy Comparison', fontsize=16)
plt.ylabel('Accuracy Score', fontsize=12)
plt.xlabel('Model', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.ylim(0.7, 1.0)
for index, value in enumerate(list(accuracies.values())):
    plt.text(index, value + 0.005, f'{value:.3f}', ha='center')
plt.tight_layout()
plt.show()

# Precision and Recall comparison
print("\n--- Precision, Recall, and Accuracy of All Models ---")
metrics_df = pd.DataFrame({
    'Accuracy': {name: res['accuracy'] for name, res in results.items()},
    'Precision': {name: res['precision'] for name, res in results.items()},
    'Recall': {name: res['recall'] for name, res in results.items()}
})
print(metrics_df.round(4))

"""## 6.2 Confusion Matrices for Each Model"""

# Confusion Matrix for each model
fig, axes = plt.subplots(1, len(models), figsize=(25, 5))
fig.suptitle('Confusion Matrices for Each Model', fontsize=16)
for i, (name, res) in enumerate(results.items()):
    sns.heatmap(res['cm'], annot=True, fmt='d', cmap='Blues', ax=axes[i], cbar=False)
    axes[i].set_title(name)
    axes[i].set_xlabel('Predicted Label')
    axes[i].set_ylabel('True Label')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""## 6.3 ROC Curve Comparison for All Models"""

# ROC Curve for each model on one plot
plt.figure(figsize=(10, 8))
for name, res in results.items():
    plt.plot(res['fpr'], res['tpr'], lw=2, label=f"{name} (AUC = {res['roc_auc']:.3f})")
plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance Level (AUC = 0.50)')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve Comparison for All Models', fontsize=16)
plt.legend(loc='lower right', fontsize=10)
plt.grid(True)
plt.show()

"""## 6.4 K-Means Clusters (Visualized with PCA)"""

print("Preprocessing data for K-Means...")
preProcessedXFull = preprocessor.fit_transform(x)

print("Applying K-Means with k=2...")
kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')
clusters = kmeans.fit_predict(preProcessedXFull)

print("K-Means clustering complete.")
print("Visualizing clusters using PCA...")
pca = PCA(n_components=2)
xPCA = pca.fit_transform(preProcessedXFull.toarray())

plt.figure(figsize=(10, 7))
scatter = plt.scatter(xPCA[:, 0], xPCA[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=10)
plt.title('K-Means Clusters (Visualized with PCA)', fontsize=16)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(handles=scatter.legend_elements()[0], labels=['Cluster 0', 'Cluster 1'])
plt.grid(True)
plt.show()

"""# **7. Testing a new booking**"""

bestModel = max(accuracies, key=accuracies.get)
bestPipeline = results[bestModel]['pipeline']
bestClassifier = bestPipeline.named_steps['classifier']
fittedPreprocessor = bestPipeline.named_steps['preprocessor']
print(f"The best performing model is '{bestModel}' with an accuracy of {accuracies[bestModel]:.3f}.")
print()

# Create a new booking
newBooking = {
    'hotel': 'City Hotel',
    'lead_time': 5,
    'arrival_date_year': 2024,
    'arrival_date_month': 'December',
    'arrival_date_week_number': 50,
    'arrival_date_day_of_month': 10,
    'stays_in_weekend_nights': 0,
    'stays_in_week_nights': 1,
    'adults': 1,
    'children': 0,
    'babies': 0,
    'meal': 'SC',
    'country': 'USA',
    'market_segment': 'Direct',
    'distribution_channel': 'Direct',
    'is_repeated_guest': 1,
    'previous_cancellations': 0,
    'previous_bookings_not_canceled': 5,
    'reserved_room_type': 'A',
    'assigned_room_type': 'A',
    'booking_changes': 1,
    'deposit_type': 'No Deposit',
    'agent': 0.0,
    'days_in_waiting_list': 0,
    'customer_type': 'Contract',
    'adr': 90.0,
    'required_car_parking_spaces': 1,
    'total_of_special_requests': 2
}

dfNew = pd.DataFrame([newBooking])
preprocessedNew = fittedPreprocessor.transform(dfNew)
predictionNew = bestClassifier.predict(preprocessedNew)
if hasattr(bestClassifier, "predict_proba"):
    predProbNew = bestClassifier.predict_proba(preprocessedNew)
else:
    predProbNew = None
    print("Note: Classifier does not support predict_proba.")


print(f"New Booking Details:\n{dfNew.T.to_string(header=False)}")


print("\nModel Prediction:")
if predictionNew[0] == 1:
    print("  -> Status: This booking is LIKELY TO BE CANCELED.")
else:
    print("  -> Status: This booking is LIKELY TO BE KEPT (NOT CANCELED).")

if predProbNew is not None:
    print("\nPrediction Probabilities:")
    print(f"  - Probability of NOT being canceled (Class 0): {predProbNew[0][0]:.2%}")
    print(f"  - Probability of BEING canceled (Class 1):    {predProbNew[0][1]:.2%}")


print("\nTesting finished.")